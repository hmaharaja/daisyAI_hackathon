{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DaisyOCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmaharaja/daisyAI_hackathon/blob/master/DaisyOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOLpr3QVkBdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kP1v9QKlhal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytesseract\n",
        "import csv\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPQIWvD4oJAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8wGZxG8lpIy",
        "colab_type": "code",
        "outputId": "89e0b635-1ae0-408f-de3b-fcd8afcf9213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# test ocr with one image \n",
        "image_path_in_colab=\"/content/drive/My Drive/Daisy hackathon 2020/files/flyer_images/week_9_page_1.jpg\"\n",
        "extractedInformation = pytesseract.image_to_string(Image.open(image_path_in_colab))\n",
        "print(extractedInformation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kok ke ke ke ke ke ke ok Ok Ok Ok Ok Ok kok\n",
            "\n",
            " \n",
            "\n",
            "t LONG LABOR DAY\n",
            "\n",
            "est 1975\n",
            "\n",
            " \n",
            "\n",
            "SAVE $1/Ib.\n",
            "Potato Salads, Cranberry Pecan\n",
            "Coleslaws, & Chicken Salad\n",
            "\n",
            "Macaroni Salad\n",
            "Select Varieties\n",
            "Made with mayo from Cage Free eggs, soy\n",
            "free, no artificial flavors, sweeteners, preservatives.\n",
            "Discount Taken at Register\n",
            "\n",
            " \n",
            "\n",
            "SAVE $2/Ib.\n",
            "\n",
            "Signature item, made fresh in-house\n",
            "in small batches. Made with American\n",
            "Humane Certified chicken that is never\n",
            "administered antibiotics or growth hormones.\n",
            "Discount Taken at Register\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "bh Ce Ce i i i i Ci i i i Ae Ce i i i Cn i i Ci in Ain, i. i Ci Ci. aie 4\n",
            "\n",
            " \n",
            "\n",
            "SAVE up to $2/Ib.\n",
            "\n",
            " \n",
            "\n",
            "SAVE $2.50/Ib.\n",
            "Boneless, Skinless Boneless Pork\n",
            "Chicken Breasts Loin Chops\n",
            "Never Administered Antibiotics oi + Never Administered Antibiotics\n",
            "\n",
            "“\n",
            "or Growth Hormones, 100% ie ee\n",
            "Vegetarian Diet, 3rd Party ~ ee se\n",
            "Certified Humanely Raised Ps:\n",
            "Discount Taken at Register\n",
            "\n",
            "or Growth Hormones, 100%\n",
            "Vegetarian Diet, Never Fed\n",
            "Ractopamine, Never Raised\n",
            "\n",
            "in Gestation Crates\n",
            "\n",
            "Discount Taken at Register\n",
            "\n",
            " \n",
            "\n",
            "Healthier for Grilling = Never Administered Antibiotics or Growth Hormones = Raised on\n",
            "100% non-GMO Grass Pastures = High in Omega-3s and Conjugated Linoleic Acid\n",
            "\n",
            "SAVE up to$4/Ib. SAVES$3/Ib. SAVE $3/Ib.\n",
            "\n",
            "Grass-Fed Grass-Fed Grass-Fed\n",
            "Ground Sirloin Boneless NY Strip\n",
            "or Round Ribeye Steaks\n",
            "\n",
            "Steak\n",
            "\n",
            "   \n",
            "\n",
            "  \n",
            "\n",
            "SAVE $3/Ib.\n",
            "\n",
            "  \n",
            "\n",
            "SAVE $2/Ib.\n",
            "Organic\n",
            "\n",
            "Red & Green\n",
            "Seedless Grapes\n",
            "\n",
            "“Known to boost an important heart\n",
            "antioxidant known as glutathione, grapes\n",
            "can protect against heart failure and high\n",
            "blood pressure.” - Angela Hind, M.D.\n",
            "and Wellness Expert\n",
            "\n",
            "Discount Taken at Register\n",
            "\n",
            "SAVE $1\n",
            "\n",
            "Oskar Blues\n",
            "\n",
            "Dale’s Pale Ale\n",
            "\n",
            "6 Pack Cans, All Varieties\n",
            "\n",
            "Metal packaging is 100% recyclable and offers\n",
            "\n",
            "a powerful barrier against light and oxygen,\n",
            "protecting the flavor and freshness of craft brews.\n",
            "$8.49 in AL, FL, & MI, Save 50¢;\n",
            "\n",
            "$9.99 in OH, Everyday Low Price\n",
            "\n",
            "Steaks\n",
            "\n",
            "  \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "SAVE $3/Ib.\n",
            "\n",
            "Steaks\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "SAVE up to $3.98 on 2\n",
            "\n",
            "Blueberries & Blackberries\n",
            "Half Pint\n",
            "\n",
            "These are the perfect size for grab-and-go snacking,\n",
            "with a burst of juicy sweetness in each morsel, and\n",
            "are a great source of Vitamin A, Vitamin C, and fiber.\n",
            "Discount Taken at Register\n",
            "\n",
            "Scie\n",
            "\n",
            "oS\n",
            "\n",
            "SAVE $2.50/Ib.\n",
            "\n",
            "SAVE $2/lb.\n",
            "Grass-Fed Grass-Fed Grass-Fed Grass-Fed\n",
            "Filet Mignon Flank Skirt Extra Lean\n",
            "Stew\n",
            "\n",
            "Mi, | Lal \\\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lJ_AZI-rGNG",
        "colab_type": "code",
        "outputId": "7b71254e-fb6f-4176-96c9-b1cdc8b0ae07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# loop through all images and save the ocr result to a text file\n",
        "\n",
        "#os.mknod(\"newfile.txt\")\n",
        "# NOTE: ONLY CREATE THE FILE ONCE, THEN DOWNLOAD IT AND USE OPEN WITHOUT WRITING TO A NEW FILE AGAIN. MAKING A NEW FILE TAKES REALLY LONG.\n",
        "f = open(\"newfile.txt\", 'w')\n",
        "for filename in os.listdir(\"/content/drive/My Drive/Daisy hackathon 2020/files/flyer_images/\"):\n",
        "  extractedInformation = pytesseract.image_to_string(Image.open(\"/content/drive/My Drive/Daisy hackathon 2020/files/flyer_images/\" + filename))\n",
        "  print(filename)\n",
        "  f.write(extractedInformation)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "week_40_page_2.jpg\n",
            "week_40_page_1.jpg\n",
            "week_3_page_4.jpg\n",
            "week_3_page_3.jpg\n",
            "week_3_page_2.jpg\n",
            "week_3_page_1.jpg\n",
            "week_39_page_2.jpg\n",
            "week_39_page_1.jpg\n",
            "week_38_page_4.jpg\n",
            "week_38_page_3.jpg\n",
            "week_38_page_2.jpg\n",
            "week_38_page_1.jpg\n",
            "week_37_page_4.jpg\n",
            "week_37_page_3.jpg\n",
            "week_37_page_2.jpg\n",
            "week_37_page_1.jpg\n",
            "week_36_page_3.jpg\n",
            "week_36_page_2.jpg\n",
            "week_36_page_1.jpg\n",
            "week_35_page_4.jpg\n",
            "week_35_page_3.jpg\n",
            "week_35_page_2.jpg\n",
            "week_35_page_1.jpg\n",
            "week_34_page_4.jpg\n",
            "week_34_page_3.jpg\n",
            "week_34_page_2.jpg\n",
            "week_34_page_1.jpg\n",
            "week_33_page_4.jpg\n",
            "week_48_page_1.jpg\n",
            "week_47_page_4.jpg\n",
            "week_47_page_3.jpg\n",
            "week_47_page_2.jpg\n",
            "week_47_page_1.jpg\n",
            "week_46_page_4.jpg\n",
            "week_46_page_3.jpg\n",
            "week_46_page_2.jpg\n",
            "week_46_page_1.jpg\n",
            "week_45_page_4.jpg\n",
            "week_45_page_3.jpg\n",
            "week_45_page_2.jpg\n",
            "week_45_page_1.jpg\n",
            "week_44_page_4.jpg\n",
            "week_44_page_3.jpg\n",
            "week_44_page_2.jpg\n",
            "week_44_page_1.jpg\n",
            "week_43_page_4.jpg\n",
            "week_43_page_3.jpg\n",
            "week_43_page_2.jpg\n",
            "week_43_page_1.jpg\n",
            "week_40_page_3.jpg\n",
            "week_39_page_4.jpg\n",
            "week_4_page_4.jpg\n",
            "week_4_page_3.jpg\n",
            "week_4_page_2.jpg\n",
            "week_4_page_1.jpg\n",
            "week_49_page_4.jpg\n",
            "week_49_page_3.jpg\n",
            "week_49_page_2.jpg\n",
            "week_49_page_1.jpg\n",
            "week_48_page_2.jpg\n",
            "week_50_page_1.jpg\n",
            "week_51_page_3.jpg\n",
            "week_51_page_2.jpg\n",
            "week_51_page_1.jpg\n",
            "week_50_page_4.jpg\n",
            "week_50_page_3.jpg\n",
            "week_50_page_2.jpg\n",
            "week_52_page_3.jpg\n",
            "week_9_page_3.jpg\n",
            "week_9_page_2.jpg\n",
            "week_9_page_1.jpg\n",
            "week_8_page_4.jpg\n",
            "week_8_page_3.jpg\n",
            "week_8_page_2.jpg\n",
            "week_8_page_1.jpg\n",
            "week_7_page_4.jpg\n",
            "week_7_page_3.jpg\n",
            "week_7_page_2.jpg\n",
            "week_7_page_1.jpg\n",
            "week_6_page_4.jpg\n",
            "week_6_page_3.jpg\n",
            "week_6_page_2.jpg\n",
            "week_6_page_1.jpg\n",
            "week_5_page_4.jpg\n",
            "week_5_page_3.jpg\n",
            "week_5_page_2.jpg\n",
            "week_5_page_1.jpg\n",
            "week_52_page_4.jpg\n",
            "week_52_page_2.jpg\n",
            "week_9_page_4.jpg\n",
            "week_51_page_4.jpg\n",
            "week_28_page_4.jpg\n",
            "week_10_page_1.jpg\n",
            "week_32_page_1.jpg\n",
            "week_30_page_2.jpg\n",
            "week_31_page_2.jpg\n",
            "week_29_page_2.jpg\n",
            "week_28_page_7.jpg\n",
            "week_28_page_6.jpg\n",
            "week_36_page_4.jpg\n",
            "week_10_page_2.jpg\n",
            "week_39_page_3.jpg\n",
            "week_48_page_3.jpg\n",
            "week_10_page_3.jpg\n",
            "week_48_page_4.jpg\n",
            "week_52_page_1.jpg\n",
            "week_10_page_4.jpg\n",
            "week_11_page_1.jpg\n",
            "week_11_page_2.jpg\n",
            "week_11_page_3.jpg\n",
            "week_11_page_4.jpg\n",
            "week_12_page_1.jpg\n",
            "week_12_page_2.jpg\n",
            "week_12_page_3.jpg\n",
            "week_12_page_4.jpg\n",
            "week_13_page_1.jpg\n",
            "week_13_page_2.jpg\n",
            "week_13_page_3.jpg\n",
            "week_13_page_4.jpg\n",
            "week_14_page_1.jpg\n",
            "week_14_page_2.jpg\n",
            "week_14_page_3.jpg\n",
            "week_14_page_4.jpg\n",
            "week_15_page_1.jpg\n",
            "week_15_page_2.jpg\n",
            "week_15_page_3.jpg\n",
            "week_15_page_4.jpg\n",
            "week_16_page_1.jpg\n",
            "week_16_page_2.jpg\n",
            "week_16_page_3.jpg\n",
            "week_16_page_4.jpg\n",
            "week_17_page_1.jpg\n",
            "week_17_page_2.jpg\n",
            "week_17_page_3.jpg\n",
            "week_17_page_4.jpg\n",
            "week_18_page_1.jpg\n",
            "week_18_page_2.jpg\n",
            "week_18_page_3.jpg\n",
            "week_18_page_4.jpg\n",
            "week_19_page_1.jpg\n",
            "week_19_page_2.jpg\n",
            "week_19_page_3.jpg\n",
            "week_19_page_4.jpg\n",
            "week_1_page_1.jpg\n",
            "week_1_page_2.jpg\n",
            "week_1_page_3.jpg\n",
            "week_1_page_4.jpg\n",
            "week_20_page_1.jpg\n",
            "week_20_page_2.jpg\n",
            "week_20_page_3.jpg\n",
            "week_20_page_4.jpg\n",
            "week_21_page_1.jpg\n",
            "week_21_page_2.jpg\n",
            "week_21_page_3.jpg\n",
            "week_21_page_4.jpg\n",
            "week_22_page_1.jpg\n",
            "week_22_page_2.jpg\n",
            "week_22_page_3.jpg\n",
            "week_22_page_4.jpg\n",
            "week_23_page_1.jpg\n",
            "week_23_page_2.jpg\n",
            "week_23_page_3.jpg\n",
            "week_23_page_4.jpg\n",
            "week_24_page_1.jpg\n",
            "week_24_page_2.jpg\n",
            "week_24_page_3.jpg\n",
            "week_24_page_4.jpg\n",
            "week_25_page_1.jpg\n",
            "week_25_page_2.jpg\n",
            "week_25_page_3.jpg\n",
            "week_25_page_4.jpg\n",
            "week_26_page_1.jpg\n",
            "week_26_page_2.jpg\n",
            "week_26_page_3.jpg\n",
            "week_26_page_4.jpg\n",
            "week_27_page_1.jpg\n",
            "week_27_page_2.jpg\n",
            "week_27_page_3.jpg\n",
            "week_27_page_4.jpg\n",
            "week_28_page_1.jpg\n",
            "week_28_page_2.jpg\n",
            "week_28_page_3.jpg\n",
            "week_28_page_5.jpg\n",
            "week_28_page_8.jpg\n",
            "week_29_page_1.jpg\n",
            "week_29_page_3.jpg\n",
            "week_29_page_4.jpg\n",
            "week_2_page_1.jpg\n",
            "week_2_page_2.jpg\n",
            "week_2_page_3.jpg\n",
            "week_2_page_4.jpg\n",
            "week_30_page_1.jpg\n",
            "week_30_page_3.jpg\n",
            "week_30_page_4.jpg\n",
            "week_31_page_1.jpg\n",
            "week_31_page_3.jpg\n",
            "week_31_page_4.jpg\n",
            "week_32_page_2.jpg\n",
            "week_32_page_3.jpg\n",
            "week_32_page_4.jpg\n",
            "week_33_page_1.jpg\n",
            "week_33_page_2.jpg\n",
            "week_33_page_3.jpg\n",
            "week_40_page_4.jpg\n",
            "week_41_page_1.jpg\n",
            "week_41_page_2.jpg\n",
            "week_41_page_3.jpg\n",
            "week_41_page_4.jpg\n",
            "week_42_page_1.jpg\n",
            "week_42_page_2.jpg\n",
            "week_42_page_3.jpg\n",
            "week_42_page_4.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnC7e0Zgn4Jr",
        "colab_type": "text"
      },
      "source": [
        "# **Fuzzy Matching Method**\n",
        " - this is just a simple brute force algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcrKz5xfoQ9-",
        "colab_type": "code",
        "outputId": "5da450c4-e61f-4c01-e0eb-412db94befba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install textdistance[extras]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.17.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (42.0.2)\n",
            "Collecting textdistance[extras]\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/18/31397b687f50ffae65469175f07faa68f288e27fcd8716276004c42e5637/textdistance-4.1.5-py3-none-any.whl\n",
            "Collecting pyxDamerauLevenshtein; extra == \"extras\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/54/2d398545cae80d2fc8444345542ad5f3ffab0694c8efb8ed2fbe92017305/pyxDamerauLevenshtein-1.5.3.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy; extra == \"extras\" in /usr/local/lib/python3.6/dist-packages (from textdistance[extras]) (1.17.5)\n",
            "Requirement already satisfied: python-Levenshtein; extra == \"extras\" in /usr/local/lib/python3.6/dist-packages (from textdistance[extras]) (0.12.0)\n",
            "Collecting abydos; extra == \"extras\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/a5/ca258a571997be1c9483d6075bbc1b9487ae80f3bb3bf1f60db0b29f5aa6/abydos-0.5.0-py2.py3-none-any.whl (886kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 5.8MB/s \n",
            "\u001b[?25hCollecting jellyfish; extra == \"extras\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein; extra == \"extras\"->textdistance[extras]) (42.0.2)\n",
            "Collecting deprecation\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2a/d5084a8781398cea745c01237b95d9762c382697c63760a95cc6a814ad3a/deprecation-2.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from deprecation->abydos; extra == \"extras\"->textdistance[extras]) (20.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->deprecation->abydos; extra == \"extras\"->textdistance[extras]) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->deprecation->abydos; extra == \"extras\"->textdistance[extras]) (2.4.6)\n",
            "Building wheels for collected packages: pyxDamerauLevenshtein\n",
            "  Building wheel for pyxDamerauLevenshtein (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyxDamerauLevenshtein: filename=pyxDamerauLevenshtein-1.5.3-cp36-cp36m-linux_x86_64.whl size=86803 sha256=a483c542fa64dd9e932fedc93611055cd055e2068c1dfbbaded2d5d759a0d0e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/66/2c/863e33407d169ea809ce2908e86556df205dcf5edc4bdd10a5\n",
            "Successfully built pyxDamerauLevenshtein\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp36-cp36m-linux_x86_64.whl size=73008 sha256=a2228b163bfa84d6d2efb2b21fecef45816cc48cbd7beed80418b0c46b283257\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: pyxDamerauLevenshtein, deprecation, abydos, jellyfish, textdistance\n",
            "Successfully installed abydos-0.5.0 deprecation-2.0.7 jellyfish-0.7.2 pyxDamerauLevenshtein-1.5.3 textdistance-4.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66sXWFhosEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "import pandas as pd\n",
        "import textdistance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyfKKEb0pYYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in open(newfile.txt):\n",
        "  # compare words using textdistance.levenshtein(\"this test\", \"that test\") - take what it has the highest similarity to "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaGEwI57oBqH",
        "colab_type": "text"
      },
      "source": [
        "# **Another Potential Method: LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2zsfKIF3Yn7",
        "colab_type": "code",
        "outputId": "a17fe2bb-560c-4106-b2f1-5605c5699426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create embeddings\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "                              dim=50)   # embedding size = 50 \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 399279/400000 [00:13<00:00, 30589.50it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgAJB7OwMOPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create/format the input\n",
        "#create a glove embedding and use that \n",
        "# save output to a csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAYdOWauQol9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4NTi061QsqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy and training/ other helper functions\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for ads, labels in data_loader:\n",
        "        output = model(ads)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def split_punctuation(ad):\n",
        "    # separate punctuations\n",
        "    ad = ad.replace(\".\", \" . \") \\\n",
        "                 .replace(\",\", \" , \") \\\n",
        "                 .replace(\";\", \" ; \") \\\n",
        "                 .replace(\"?\", \" ? \") \\\n",
        "                 .replace(\"!\", \" ! \") \\\n",
        "                 .replace(\"|\", \" | \") \\\n",
        "                 .replace(\"$\", \" $ \") \\\n",
        "                 .replace(\"%\", \" % \") \\\n",
        "                 .replace(\"&\", \" & \")\n",
        "    return ad.lower().split()\n",
        "\n",
        "\n",
        "def train_network(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, val_losses, train_acc, valid_acc = [], [], [], []\n",
        "    epochs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for ads, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(ads)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        losses.append(float(loss))\n",
        "\n",
        "        for val_batch in valid_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(ads)\n",
        "            val_loss = criterion(pred, label)\n",
        "            val_loss.backward()\n",
        "            optimizer.step()\n",
        "        val_losses.append(float(val_loss))\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"Epoch %d; Training Loss %f; Val Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
        "    # plotting\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, losses, label=\"Train\")\n",
        "    plt.plot(epochs, val_losses, label = 'Validation')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MNyAVOmfSzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model architechture: LSTM\n",
        "\n",
        "# What is the right syntax for the output \"out\" - we want it to be a vector? What is the shape and how do we get it?\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the LSTM\n",
        "        out, _ = self.rnn(x, h0, c0)\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}